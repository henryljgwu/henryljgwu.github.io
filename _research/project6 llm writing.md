---
title: "Research idea: Can LLM workflow with agent and RAG write a good story?"
date: 2024-08-06
priority: 2
---

<!--
1. LLM本身不擅长 长文的写作，以及需要大量参考事实的写作，更重要的是其注意力机制导致其在很长的窗口上表现不佳。但是在近年的新兴的扩展context窗口，多轮对话，agent，以及不断迭代的rag技术或许能
2. 定义标准：也许可以和之前的llm automated benchmark相结合
3. 展示LLM本地的坏的写作成绩
4. 展示固定架构的LLM workflow
5. 展示 workflow成绩
6. 与sota相比
-->

# Exploring the Potential of LLM Workflow with Agent and RAG in Story Writing

---

# Introduction

Large Language Models (LLMs) have demonstrated impressive capabilities across a range of natural language processing tasks. However, they exhibit notable limitations when it comes to writing long-form content, especially when the content requires substantial reference to factual information. The inherent attention mechanism of LLMs, which struggles with very long context windows, exacerbates these challenges. Recently, advancements in extended context windows, multi-turn dialogues, agent frameworks, and iterative Retrieval-Augmented Generation (RAG) techniques have emerged, offering potential solutions to these issues. This research seeks to explore whether integrating these technologies into an LLM workflow can significantly enhance the model's ability to write coherent and factually accurate stories.

# Definition of Evaluation Standards

To assess the effectiveness of the proposed LLM workflow, we must first establish a set of evaluation criteria. These standards will be informed by existing benchmarks for LLM performance, such as the LLM Automated Benchmark (LLMAB). By combining these benchmarks with specific metrics tailored to long-form story writing—such as coherence, narrative structure, factual accuracy, and creativity—we can rigorously evaluate the quality of the stories generated by the LLM workflow. These criteria will serve as the foundation for comparing the proposed workflow's performance against both traditional LLM outputs and state-of-the-art (SOTA) methods.

# Baseline Performance: LLM's Native Story Writing Capabilities

To establish a baseline for evaluating the effectiveness of the proposed workflow, we first assess the performance of an LLM in generating long-form stories without any enhancements or modifications. This baseline serves as a critical reference point, highlighting the inherent challenges that LLMs face in long-form content generation.

**Model Selection**

We use a standard, pre-trained LLM, such as llama-3.1-8b-instruct / llama-3.1-70b-instruct, known for its wide adoption and robust performance across various tasks. The model is tested without any fine-tuning or auxiliary mechanisms that might aid in long-form generation.

**Task Setup**

The LLM is tasked with generating a short novel or an extended narrative on a given topic. The input prompt is designed to be brief, providing minimal guidance, thus challenging the model's ability to maintain coherence and factual accuracy over long outputs.

**Evaluation Metrics**

- **Coherence**: We assess the narrative flow and logical progression within the story.
- **Factual Accuracy**: The model's ability to incorporate accurate information, especially when the story requires referencing factual content.
- **Creativity**: The originality and engagement level of the content produced.
- **Attention Span**: Measured by how well the model maintains focus and relevance as the story progresses, particularly in the later sections.

**Expected Results**

The native LLM should demonstrates several weaknesses:
- **Coherence Decay**: The story should often loses logical consistency, especially as the narrative progresses. This is attributed to the model's limited attention span, which struggles with very long context windows.
- **Hallucinations**: The model should generates some content that is factually incorrect or irrelevant to the initial prompt, a common issue known as "hallucination" in LLMs.
- **Repetitiveness**: There should be a tendency to repeat ideas or phrases, reflecting the model's difficulty in sustaining fresh and diverse content over extended text.

These observations should be consistent with findings from recent studies, such as those documented in the "Lost in the Middle" paper, which examines the challenges of maintaining relevance and coherence in long-context tasks using LLMs ([source](https://arxiv.org/abs/2307.03172)).

# Design and Implementation of the LLM Workflow

To address the limitations identified in the baseline, we propose a novel workflow that leverages recent advancements in LLM technology, specifically focusing on extended context windows, agent-based frameworks, and Retrieval-Augmented Generation (RAG).

**Workflow Components**

- **Extended Context Windows**: 
  We utilize LLMs with enhanced context windows, such as those provided by models like Llama 3 or specialized long-context models like NeedleBench, which can manage significantly larger context sizes ([source](https://github.com/Xnhyacinth/Awesome-LLM-Long-Context-Modeling)). These models are designed to better handle the dependencies and coherence challenges over long sequences, crucial for maintaining narrative consistency.

- **Agent-Based Framework**: 
  The workflow integrates an agent that dynamically interacts with the LLM during the story generation process. The agent's role is to monitor narrative coherence, prompt the LLM to revisit or revise sections as needed, and manage the flow of information between different parts of the story. This approach is inspired by frameworks like those discussed in "Synergistic Multi-Agent Frameworks," which emphasize collaboration between multiple specialized agents to improve task performance ([source](https://arxiv.org/abs/2307.09488)).

- **Retrieval-Augmented Generation (RAG)**: 
  RAG is incorporated to enhance the factual accuracy of the content. The LLM retrieves relevant information from a pre-defined corpus or external databases in real-time, allowing it to incorporate accurate details as the story unfolds ([source](https://arxiv.org/abs/2308.14544)). This component is particularly useful for integrating complex information that spans across different parts of the narrative, ensuring that the model can reference accurate data points and maintain a logical storyline.

**Implementation Pipeline**

- **Initial Prompt and Context Setup**: 
  The story begins with a well-defined prompt that includes an initial context, narrative direction, and key elements that need to be incorporated. The agent assesses this input and identifies potential challenges in maintaining coherence over the expected length.

- **Iterative Generation with RAG**: 
  The LLM generates the story in segments, with each segment being immediately reviewed by the agent for coherence and factual accuracy. If discrepancies or hallucinations are detected, the agent triggers a retrieval process to gather relevant information, which is then used to revise the segment. The iterative process ensures that the story remains on track and that factual information is correctly integrated.

- **Final Review and Refinement**: 
  Once the story is fully generated, the agent conducts a final review to identify any lingering issues in coherence or accuracy. Any sections that require further refinement are revised before the story is finalized.

**Expected Results**

By combining these advanced techniques, we should anticipate significant improvements in the quality of long-form content generated by LLMs. The extended context windows should mitigate the coherence decay observed in the baseline, while the agent and RAG components work together to enhance the narrative structure and factual reliability. This approach aligns with the latest SOTA methodologies for long-form content generation, providing a robust framework for future development and research.



# Evaluation of Workflow Performance

Waiting for experiement to implement. ~~GPU is needed~~

# Comparison with State-of-the-Art Methods

To contextualize the results, the performance of the LLM workflow will be compared with current state-of-the-art (SOTA) methods in long-form content generation. This comparison will provide insights into how well the workflow stacks up against the latest advancements in the field and whether it offers a viable alternative or complement to existing techniques. The discussion will also explore potential avenues for integrating the LLM workflow with other SOTA methods to further enhance story writing capabilities.


